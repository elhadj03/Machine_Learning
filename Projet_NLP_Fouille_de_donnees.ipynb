{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o9PnLG65P0Ws"
   },
   "source": [
    "##  Projet HMIN326 -- Fouille de données\n",
    "\n",
    "# Classification de documents par opinion\n",
    "Encadrement :\n",
    "Dino Ienco, Konstantin Todorov, Pascal Poncelet\n",
    "Octobre 2020\n",
    "\n",
    "Le but de ce projet consiste à mettre en oeuvre et évaluer des méthodes de classification de documents par opinion.\n",
    "Le corpus\n",
    "\n",
    "Un jeu de données textuelles nous est mis à disposition. Il s'agit d'un corpus de 10000 documents contenant des avis d'internautes sur des films. A chaque document est associé sa polarité selon l'avis (+1 : positif, -1 : négatif). Le fichier des documents est formaté dans un tableau cvs (un avis par ligne), un autre fichier csv contient les polarités d'avis par document (- 1/+1). Une correspondance directe existe entre les numéros des lignes des documents et les polarités.\n",
    "\n",
    "## Etape 1 : Transformation des données\n",
    "\n",
    "On utilise Scikit Learn à la place de WEKA pour effectuer les transformations et vectorisations donc pas besoin de transformer en .arff comme pour le cas WEKA.Par la suite, les valeurs textuelles doivent être rendues numériques en utilisant une pondération fréquentielle (tf-idf, tf, ou autres). normalisation !.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7VX_iavrOXbo"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import time\n",
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "import string \n",
    "import matplotlib.pyplot as plt\n",
    "#Pour afficher les nuages de donnees\n",
    "from wordcloud import WordCloud\n",
    "import seaborn as sns\n",
    "#from nltk.tokenize import word_tokenize \n",
    "from nltk import pos_tag,word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.corpus import wordnet \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem.porter import PorterStemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nwhl6SiKlZJN",
    "outputId": "194802ec-d1f0-490f-8caa-e5f9822cae4c"
   },
   "outputs": [],
   "source": [
    "#permet de telecharger tous les bibliothèques\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('omw')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lPj9HmKUTjzD",
    "outputId": "f120dbd7-9938-4998-c85d-a950dc69abb3"
   },
   "outputs": [],
   "source": [
    "# Creation datafrane a partir de fichiers csv \n",
    "Dataset=pd.read_csv('dataset.csv', sep='\\t', index_col=False, header = None)\n",
    "Labels = pd.read_csv('labels.csv', sep='\\t', index_col=False, header = None)\n",
    "pd.options.display.max_colwidth = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset.info()\n",
    "Dataset.head(10)  # si on veut afficher 15 lignes sinon 5 par defaut .head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Labels.info()\n",
    "Labels.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affichage et exploration du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qq_8g0uoXtKZ",
    "outputId": "b54d2161-8cd5-46fa-ce2b-91b5ed6dfcfb"
   },
   "outputs": [],
   "source": [
    "\"\"\" Check for missing values: Machine learning models usually require complete data. \"\"\"\n",
    "\n",
    "#retourne le nombre de valeurs manquantes. refere aux cellules vides ou donnees absentes selon le modele \n",
    "#https://miamioh.instructure.com/courses/38817/pages/data-cleaning pour plus de comprehension a mettre en annexe\n",
    "Dataset.isnull().sum()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8LUUchTZQW8"
   },
   "source": [
    "#### Concatenation of the two files dataset and labels( in positive and negative )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gfjZlhAQfEC_"
   },
   "outputs": [],
   "source": [
    "#Pour augmenter la largeur d'affichage\n",
    "pd.set_option('display.max_columns', 350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "id": "BsiDS2hwZPsS",
    "outputId": "fe5523a8-1b9f-4d86-eaec-aa5d4df68049"
   },
   "outputs": [],
   "source": [
    "#la concatenation va nous permettre de travailler sur un meme fichier, il suffit juste d'appeler la colonne concernee\n",
    "fullDataset=pd.concat([Dataset,Labels],axis=1,ignore_index=True,verify_integrity=True)\n",
    "fullDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8WHWGrzIX0vp",
    "outputId": "2b2f27b4-83d6-4683-d405-0a430fc18c3c"
   },
   "outputs": [],
   "source": [
    "fullDataset=fullDataset.rename(columns={0:'comment', \n",
    "                                        1:'labels'\n",
    "                                        })\n",
    "fullDataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "Hb53XmGDf_y4",
    "outputId": "f214d0cb-e5fb-430a-c19b-0ec8101cf9f6"
   },
   "outputs": [],
   "source": [
    "Features=['comment','labels']\n",
    "full_Dataset=pd.DataFrame(fullDataset,columns=Features)\n",
    "full_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "id": "-FKHBAx2gCVP",
    "outputId": "1f1d83c1-96f2-4cc0-d319-91e453b87511"
   },
   "outputs": [],
   "source": [
    "#compte les categories de labels\n",
    "fullDataset['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation graphique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "sns.set(style = \"darkgrid\" , font_scale = 1.2)\n",
    "sns.countplot(x=fullDataset.labels, y=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Afficher les mots positifs de notre dataset sous format nuages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wordcloud permet de générer les nuages de mot\n",
    "#En utilisant le package Wordcloud , nous pourrons générer une image qui nous donne les mots les plus représentatifs \n",
    "#dans un ensemble de critiques choisi. Ici nous avons choisi les 150 mots les plus répétés\n",
    "positive_values = fullDataset[(fullDataset.comment.notnull()) & (fullDataset.labels == 1)]\n",
    "wordcloud = WordCloud(width=500,height=250, max_font_size=80, max_words=150, background_color=\"white\").generate(positive_values.comment[5000])\n",
    "\n",
    "f = plt.figure() \n",
    "f.set_figwidth(15) \n",
    "f.set_figheight(10)\n",
    "\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.margins(x=0, y=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation des mots négatifs sous format nuages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_values = fullDataset[(fullDataset.comment.notnull()) & (fullDataset.labels == -1)]\n",
    "\n",
    "wordcloud = WordCloud(width=500,height=250, max_font_size=80, max_words=150, background_color=\"white\").generate(negative_values.comment[0])\n",
    "\n",
    "f = plt.figure() \n",
    "f.set_figwidth(15) \n",
    "f.set_figheight(10)\n",
    "\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.margins(x=0, y=0)\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 2 : Prétraitements des documents\n",
    "Vous utiliserez les différents types de données d'entrée selon les prétraitements. ​Le but est d'utiliser vos textes avec différentes informations, en préparant 3 versions du corpus :\n",
    "(1) Textes bruts (avec ou sans suppression de stop-words),\n",
    "(2) Textes lemmatisés,\n",
    "(3) Textes lemmatisés avec analyse morphosyntaxique (à l'aide de l'outil Tree-tagger vu en cours).\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "* corriger les contractions\n",
    "* enlever les caracteres inutiles\n",
    "* considerer que les mots alphanumeriques\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZHL3qRGtjFRo",
    "outputId": "39bde15d-7adf-4e8a-dcce-4dd5d55d2195"
   },
   "outputs": [],
   "source": [
    "# Check for whitespace strings (it's OK if there aren't any!):\n",
    "blanks = []  # start with an empty list\n",
    "\n",
    "for index,message,labels in full_Dataset.itertuples():  # iterate over the DataFrame\n",
    "    if type(labels)==str:            # avoid NaN values\n",
    "        if labels.isspace():         # test 'review' for whitespace\n",
    "            blanks.append(i)     # add matching index numbers to the list\n",
    "        \n",
    "print(len(blanks))\n",
    "\n",
    "# le resultat de la longueur de la liste blanks est 0 donc pas de whitespace\n",
    "\" =============================================== \"\n",
    "\n",
    "text = fullDataset['comment']\n",
    "\n",
    "#corriger les contractions\n",
    "import contractions\n",
    "comments=text.apply(contractions.fix)\n",
    "\n",
    "#expression reguliere qui remplace les caracteres differents de A a z - ' et espace par rien('') \n",
    "#ou plutot un espace afin d'eviter deux mots separes par une ponctuation d'etre colles\n",
    "#les whitespaces ou espace > 1 par 1 seul espace\n",
    "#On pouvait transformer les valeurs numeriques en texte mais c'est pas interessant\n",
    "#garder les - pour les mots composes\n",
    "comments = comments.str.replace('[^A-z -]',' ').str.replace(' +',' ').str.strip()\n",
    "\n",
    "#splitwords = [ nltk.word_tokenize( str(message) ) for message in messages ]  # une facon de tokeniser\n",
    "#print(splitwords)  # ne s'affiche pas trop large\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Tokenisation du texte brut "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_bruts = comments.copy().apply(word_tokenize)  # une seconde maniere d'appliquer le meme outil\n",
    "comments_bruts.head()                                  # par defaut les 5 premieres lignes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Texte brut avec suppression stop-words"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "* supprimer les stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importer les stop words en anglais \n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "new_words = ('us','oh')\n",
    "for word in new_words:\n",
    "   stopwords.append(word) #personnalisation de nos stopwords\n",
    "\n",
    "def supprime_sw_and_nonalpha(text): # meme chose que la fonction remove_stopwords\n",
    "    return [word for word in text if word.lower() not in stopwords and word.isalpha()] \n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = []\n",
    "    for word in text:\n",
    "            if word.lower() not in stopwords:\n",
    "                words.append(word)\n",
    "\n",
    "    return words\n",
    "\n",
    "\n",
    "comments_bruts_sans_sw = comments.copy().apply(word_tokenize).apply(remove_stopwords)\n",
    "#messages_bruts_sans_sw = messages_bruts_sans_sw.apply(lambda sentence: [item for item in sentence if item not in stopwords])\n",
    "comments_bruts_sans_sw.head()                         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatisation avec wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k9ztpZVqhjbz",
    "outputId": "b9239a40-cdba-43a4-f46c-0ba4fff6a58e"
   },
   "outputs": [],
   "source": [
    "# lemmatisation en considerant les noms, adjectifs, adverbes et verbes\n",
    "\n",
    "# lemmatise le mot word selon sa categorie de tag mais pas efficace, \n",
    "#ignore des mots plutot definir une fonction pour chaque type de tag\n",
    "def lemmatize_withtag(word,tag):   \n",
    "    wn = WordNetLemmatizer()\n",
    "    if tag.startswith(\"NN\"):  #noun\n",
    "        return wn.lemmatize(word, pos='n')\n",
    "    elif tag.startswith('VB'): #verbe\n",
    "        return wn.lemmatize(word, pos='v')\n",
    "    elif tag.startswith('JJ'): #adjectif\n",
    "        return wn.lemmatize(word, pos='a')\n",
    "    elif tag.startswith('RB'): #adverbe\n",
    "        return wn.lemmatize(word, pos='r')\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "#la librairie pos_tag renvoie la liste des mots d'un texte avec la categorie de tag pour chaque\n",
    "def lemmatize_text_verbe(text):\n",
    "    return [wn.lemmatize(word, pos='v') for word in text]\n",
    "def lemmatize_text_nom(text):\n",
    "    return [wn.lemmatize(word, pos='n') for word in text]\n",
    "def lemmatize_text_adj(text):\n",
    "    return [wn.lemmatize(word, pos='a') for word in text]\n",
    "def lemmatize_text_adv(text):\n",
    "    return [wn.lemmatize(word, pos='r') for word in text]\n",
    "\n",
    "comments_lemmes_wnet = comments_bruts_sans_sw.copy().apply(lemmatize_text_verbe)\n",
    "comments_lemmes_wnet = comments_lemmes_wnet.apply(lemmatize_text_nom)\n",
    "comments_lemmes_wnet = comments_lemmes_wnet.apply(lemmatize_text_adj)\n",
    "comments_lemmes_wnet = comments_lemmes_wnet.apply(lemmatize_text_adv)\n",
    "\n",
    "\n",
    "comments_lemmes_wnet.head()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatisation avec la librairie spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "id": "eAxYdGXCjFNl",
    "outputId": "135202ba-474e-4331-e146-8db14175482d"
   },
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "doc = nlp(u\"A three movie I really wanted to love was terrible. \\\n",
    "I'm sure the producers had the best intentions, but the execution was lacking.\")\n",
    "displacy.render(doc, style='ent', jupyter=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "AJ12ygf1jFF5",
    "outputId": "a6ccb5a7-0f47-4b0f-9203-101b3b4e5e8b"
   },
   "outputs": [],
   "source": [
    "#cette partie de code mets du temps a l'execution. Patience !\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Import the displaCy library\n",
    "from spacy import displacy\n",
    "\n",
    "comments_lemmes_spacy = comments.copy().apply(nlp)\n",
    "\n",
    "#affiche le resultat pour les 5 premieres lignes\n",
    "for comment in comments_lemmes_spacy.head():\n",
    "    displacy.render(comment, style='ent', jupyter=True)\n",
    "    #print([token.lemma_ for token in message])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(messages_lemmes_spacy.head(), style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fP3g-ZLvjFnB"
   },
   "source": [
    "## Etape 3 : Mise en oeuvre d'algorithmes de classification\n",
    "La suite du travail consistera à utiliser Weka et à évaluer rigoureusement les résultats de classification obtenus en prenant en entrée les différents corpus préparés dans l'étape précédent. Rappelons que de nombreuses approches d'apprentissage peuvent alors être utilisées pour la classification de textes :\n",
    "• K plus proches voisins,\n",
    "• Arbres de décisions,\n",
    "• Naïve Bayes,\n",
    "• Machines à support de vecteurs \n",
    "• Les règles d'association\n",
    "• Ensemble classifier\n",
    "\n",
    "Paramétrage : Pour chaque méthode de classification, il existe plusieurs paramètres à choisir, tels que le paramètre K de l'algorithme des KPPV, le noyau pour les SVM, le support pour les règles, etc.\n",
    "\n",
    "Dans le cas de Scikit Learn, il est possible d’utiliser la fonction gridsearchCV pour tester différents classifieurs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# librairie pour la classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into train & test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gf0AE1j2jKGU"
   },
   "outputs": [],
   "source": [
    "X = fullDataset['comment']  # full_Dataset['comment'] after tokenisation this time we want to look at the text\n",
    "y = fullDataset['labels']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "X_train_t, X_test_t, y_train, y_test = train_test_split(X.apply(word_tokenize), y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VFv6uLNzjagK"
   },
   "source": [
    "# Scikit-learn's CountVectorizer\n",
    "Text preprocessing, tokenizing and the ability to filter out stopwords are all included in CountVectorizer, which builds a dictionary of features and transforms documents to feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1erzQa4Qj4nN"
   },
   "source": [
    "This shows that our training set is comprised of 6700 documents, and 42772 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vlGdMjOfkY80"
   },
   "source": [
    "### Transform Counts to Frequencies with Tf-idf\n",
    "While counting words is helpful, longer documents will have higher average count values than shorter documents, even though they might talk about the same topics.\n",
    "To avoid this we can simply divide the number of occurrences of each word in a document by the total number of words in the document: these new features are called tf for Term Frequencies.\n",
    "Another refinement on top of tf is to downscale weights for words that occur in many documents in the corpus and are therefore less informative than those that occur only in a smaller portion of the corpus.\n",
    "This downscaling is called tf–idf for “Term Frequency times Inverse Document Frequency”.\n",
    "Both tf and tf–idf can be computed as follows using TfidfTransformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vZAbrCrrjlhl"
   },
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape\n",
    "print(X_train_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6qI4mSFkrzO"
   },
   "source": [
    "### Combine Steps with TfidVectorizer\n",
    "In the future, we can combine the CountVectorizer and TfidTransformer steps into one using TfidVectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fgI94-0RkfXL"
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train) # remember to use the original X_train set\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Phvbhy8s5qOu"
   },
   "source": [
    "# Train  Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CrF9R_Nc6V2I"
   },
   "source": [
    "### *** Earlier we named our SVC classifier svc_model. Here we're using the more generic name clf (for classifier)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3P2CK56T7OTi"
   },
   "source": [
    "# Test classifiers and display results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Machines à support de vecteurs \n",
    "Here we'll introduce an SVM classifier that's similar to SVC, called LinearSVC. LinearSVC handles sparse input better, and scales well to large numbers of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9bP03Cx64Op"
   },
   "source": [
    "####  Build a Pipeline\n",
    "Remember that only our training set has been vectorized into a full vocabulary. In order to perform an analysis on our test set we'll have to submit it to the same procedures. Fortunately scikit-learn offers a Pipeline class that behaves like a compound classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XIgEbv1l5xFk"
   },
   "outputs": [],
   "source": [
    "\n",
    "text_clf = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('text_clf', LinearSVC()),\n",
    "])\n",
    "\n",
    "# Feed the training data through the pipeline\n",
    "text_clf.fit(X_train, y_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i4mp8f8O7AjG"
   },
   "outputs": [],
   "source": [
    "# Form a prediction set\n",
    "predictions = text_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "shbJ0ApO7cOJ"
   },
   "outputs": [],
   "source": [
    "# Report the confusion matrix\n",
    "print(metrics.confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dIJ_17PE7gXd"
   },
   "outputs": [],
   "source": [
    "# Print a classification report\n",
    "print(metrics.classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ub1TJhq7o_s"
   },
   "outputs": [],
   "source": [
    "# Print the overall accuracy\n",
    "acc1 = metrics.accuracy_score(y_test,predictions)\n",
    "print('The prediction score :',acc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RhUGv4eoA8W9"
   },
   "source": [
    "## ==========  Sentence of test  ======="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bkmPlMf_A7o0"
   },
   "outputs": [],
   "source": [
    "myreview = \"A movie I really wanted to love was terrible. \\\n",
    "I'm sure the producers had the best intentions, but the execution was lacking.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wyK7b26uA76W"
   },
   "outputs": [],
   "source": [
    "print(text_clf.predict([myreview]))  # be sure to put \"myreview\" inside square brackets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NGmYsfB7BQOs"
   },
   "source": [
    "#### interpretation\n",
    "[-1] means that the opinion is negative  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7H48-r7eCl4c"
   },
   "outputs": [],
   "source": [
    "myreview2 = \"A movie I really liked.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F_443rCbCm-E"
   },
   "outputs": [],
   "source": [
    "print(text_clf.predict([myreview2]))  # be sure to put \"myreview\" inside square brackets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p6grzPOtDnvi"
   },
   "source": [
    "[1] means that the opinion is positive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r1u3ceNb8Eug"
   },
   "source": [
    "### 2- K plus proches voisins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TeahKQUm8Yym"
   },
   "outputs": [],
   "source": [
    "text_clf = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('text_clf', KNeighborsClassifier()),\n",
    "])\n",
    "\n",
    "# Feed the training data through the pipeline\n",
    "text_clf.fit(X_train, y_train)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KWgNkONR8Y7M"
   },
   "outputs": [],
   "source": [
    "predictions = text_clf.predict(X_test)\n",
    "print(metrics.confusion_matrix(y_test,predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o-qeUXUkvZAS"
   },
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XNkOE_f1MG-_"
   },
   "outputs": [],
   "source": [
    "acc2 = metrics.accuracy_score(y_test,predictions)\n",
    "print('The prediction score :',acc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r1u3ceNb8Eug"
   },
   "source": [
    "### 3 - Arbre de decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TeahKQUm8Yym"
   },
   "outputs": [],
   "source": [
    "text_clf = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('text_clf', DecisionTreeClassifier()),\n",
    "])\n",
    "\n",
    "# Feed the training data through the pipeline\n",
    "text_clf.fit(X_train, y_train)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KWgNkONR8Y7M"
   },
   "outputs": [],
   "source": [
    "predictions = text_clf.predict(X_test)\n",
    "print(metrics.confusion_matrix(y_test,predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o-qeUXUkvZAS"
   },
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XNkOE_f1MG-_"
   },
   "outputs": [],
   "source": [
    "acc3 = metrics.accuracy_score(y_test,predictions)\n",
    "print('The prediction score :',acc3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4i1dNlRl8Rzu"
   },
   "source": [
    "### 4 - Train a naïve Bayes classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I-q6FdmZ8arT"
   },
   "outputs": [],
   "source": [
    "# Naïve Bayes:\n",
    "text_clf = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('text_clf', MultinomialNB()),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wLLYt05a73f1"
   },
   "outputs": [],
   "source": [
    "# Run predictions and analyze the results (naïve Bayes) \n",
    "text_clf.fit(X_train, y_train)\n",
    "# Form a prediction set\n",
    "predictions = text_clf.predict(X_test)\n",
    "# Report the confusion matrix\n",
    "print(metrics.confusion_matrix(y_test,predictions))\n",
    "# Print a classification report\n",
    "print(metrics.classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tlAVah9Z_ytd"
   },
   "outputs": [],
   "source": [
    "# Print the overall accuracy\n",
    "acc5 = metrics.accuracy_score(y_test,predictions)\n",
    "print('The prediction score :',acc4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4i1dNlRl8Rzu"
   },
   "source": [
    "### 5 - LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I-q6FdmZ8arT"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "text_clf = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('text_clf', LogisticRegression()),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wLLYt05a73f1"
   },
   "outputs": [],
   "source": [
    "# Run predictions and analyze the results (naïve Bayes) \n",
    "text_clf.fit(X_train, y_train)\n",
    "# Form a prediction set\n",
    "predictions = text_clf.predict(X_test)\n",
    "# Report the confusion matrix\n",
    "print(metrics.confusion_matrix(y_test,predictions))\n",
    "# Print a classification report\n",
    "print(metrics.classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tlAVah9Z_ytd"
   },
   "outputs": [],
   "source": [
    "# Print the overall accuracy\n",
    "acc5 = metrics.accuracy_score(y_test,predictions)\n",
    "print('The prediction score :',acc5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4i1dNlRl8Rzu"
   },
   "source": [
    "### 6 - Ensemble Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I-q6FdmZ8arT"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "voting_clf = VotingClassifier(estimators=[('SVM', LinearSVC()), ('DTree', DecisionTreeClassifier()), ('KPPVoisin', KNeighborsClassifier()), ('NaiveBaye', MultinomialNB())], voting='hard')\n",
    "voting_clf = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('voting_clf', MultinomialNB()),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wLLYt05a73f1"
   },
   "outputs": [],
   "source": [
    "# Run predictions and analyze the results (naïve Bayes) \n",
    "voting_clf.fit(X_train, y_train)\n",
    "# Form a prediction set\n",
    "predictions = voting_clf.predict(X_test)\n",
    "# Report the confusion matrix\n",
    "print(metrics.confusion_matrix(y_test,predictions))\n",
    "# Print a classification report\n",
    "print(metrics.classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tlAVah9Z_ytd"
   },
   "outputs": [],
   "source": [
    "# Print the overall accuracy\n",
    "acc6 = metrics.accuracy_score(y_test,predictions)\n",
    "print('The prediction score :',acc6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparaison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(figsize=(7,5))\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "models = ['SVM', 'K_PP_Voisin ', 'Dtree ', 'NaiveBaye ' , ' L_Reg ','Classifier_Set ']\n",
    "accurisy = [acc1*100 ,acc2*100,acc3*100,acc4*100,acc5*100,acc6*100]\n",
    "ax.bar(models,accurisy,color = 'bgmrc',width = 0.8) \n",
    "plt.xlabel(\"Classifier model\",size=15)\n",
    "plt.ylabel(\"Accuracy\",size=15)\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nxZE2df08bXI"
   },
   "source": [
    "# Conclusion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Projet_NLP_Fouille_de_donnees.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}